{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Initialize the LLM\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "folder_path = \"document\"  # Replace with the path to your folder containing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_txt_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all TXT files from a specified folder and return them as a list of LangChain Documents.\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the TXT files.\n",
    "    Returns:\n",
    "        List[Document]: List of LangChain Documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Read text content from the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text_content = file.read()\n",
    "            # Create a LangChain Document for each file\n",
    "            doc = Document(page_content=text_content,\n",
    "                           metadata={\"source\": file_name})\n",
    "            documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Implement Small2Big Chunking ---\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def small2big_chunking(documents, min_chunk_size=200, max_chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Apply the Small2Big chunking strategy to a list of documents and provide statistics.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of documents to chunk.\n",
    "        min_chunk_size (int): Minimum chunk size.\n",
    "        max_chunk_size (int): Maximum chunk size.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    chunked_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=min_chunk_size, chunk_overlap=50)\n",
    "\n",
    "    total_original_length = 0\n",
    "    total_chunked_length = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        # Keep track of the original length of the document\n",
    "        total_original_length += len(doc.page_content)\n",
    "\n",
    "        # Split into small chunks first\n",
    "        small_chunks = text_splitter.split_text(doc.page_content)\n",
    "        combined_chunk = \"\"\n",
    "\n",
    "        for chunk in small_chunks:\n",
    "            if len(combined_chunk) + len(chunk) <= max_chunk_size:\n",
    "                combined_chunk += \" \" + chunk\n",
    "            else:\n",
    "                chunked_docs.append(\n",
    "                    Document(page_content=combined_chunk.strip(), metadata=doc.metadata))\n",
    "                combined_chunk = chunk\n",
    "\n",
    "        # Add the last chunk if it's not empty\n",
    "        if combined_chunk:\n",
    "            chunked_docs.append(\n",
    "                Document(page_content=combined_chunk.strip(), metadata=doc.metadata))\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_chunked_length = sum(len(doc.page_content) for doc in chunked_docs)\n",
    "    avg_chunk_size = total_chunked_length / \\\n",
    "        len(chunked_docs) if chunked_docs else 0\n",
    "\n",
    "    print(\"=== Small2Big Chunking Statistics ===\")\n",
    "    print(f\"Total Original Documents: {len(documents)}\")\n",
    "    print(f\"Total Original Length: {total_original_length} characters\")\n",
    "    print(f\"Total Chunks Created: {len(chunked_docs)}\")\n",
    "    print(f\"Total Chunked Length: {total_chunked_length} characters\")\n",
    "    print(f\"Average Chunk Size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Minimum Chunk Size: {min_chunk_size} characters\")\n",
    "    print(f\"Maximum Chunk Size: {max_chunk_size} characters\")\n",
    "\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Multi-Query Expansion ---\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "    paraphrased_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique paraphrasing of the original question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "system = \"\"\"\n",
    "You are an expert in query paraphrasing and expansion. \n",
    "\n",
    "Your task is to generate multiple different phrasings of the same user query. \n",
    "Ensure that each paraphrased query captures the original meaning while using different wording.\n",
    "\n",
    "If there are multiple common ways to phrase the question, or common synonyms for key terms, ensure all are included.\n",
    "\n",
    "You **must** return at least 3 distinct paraphrased versions of the input query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_analyzer = prompt | llm_with_tools | PydanticToolsParser(tools=[\n",
    "                                                               ParaphrasedQuery])\n",
    "\n",
    "\n",
    "def expand_query(query):\n",
    "    \"\"\"\n",
    "    Generate multiple semantically similar queries using an LLM.\n",
    "    Args:\n",
    "        query (str): Original query.\n",
    "    Returns:\n",
    "        List[str]: List of expanded queries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = query_analyzer.invoke({\"question\": query})\n",
    "        # Extract paraphrased queries\n",
    "        expanded_queries = [\n",
    "            q.paraphrased_query for q in response if isinstance(q, ParaphrasedQuery)]\n",
    "\n",
    "        if not expanded_queries or len(expanded_queries) < 3:\n",
    "            print(\n",
    "                \"Warning: Less than 3 paraphrased queries returned. Consider adjusting the prompt.\")\n",
    "\n",
    "        print(f\"Expanded Queries: {expanded_queries}\\n\")\n",
    "        return expanded_queries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query expansion: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Combine Results from Multi-Query Retrieval ---\n",
    "def multi_query_retrieval(expanded_queries, retriever, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform retrieval using multiple queries and combine results.\n",
    "    \"\"\"\n",
    "    combined_results = []\n",
    "    for query in expanded_queries:\n",
    "        results = retriever.get_relevant_documents(query)[:top_k]\n",
    "        combined_results.extend(results)\n",
    "\n",
    "    # Deduplicate results based on document content\n",
    "    unique_docs = {doc.page_content: doc for doc in combined_results}\n",
    "    return list(unique_docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def rank_with_cove(query, retrieved_docs, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Rank retrieved documents based on relevance using embeddings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        query (str): The user's input query.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        retrieved_docs (List[Document]): List of retrieved documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        embeddings: The embedding model used for similarity calculations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        top_k (int): The number of top documents to return.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        List[Document]: The top-ranked documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Generate embedding for the query\n",
    "\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "\n",
    "    # Step 2: Compute embeddings for the retrieved documents\n",
    "\n",
    "    doc_contents = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "    doc_embeddings = embeddings.embed_documents(doc_contents)\n",
    "\n",
    "\n",
    "    # Step 3: Calculate cosine similarity between query and document embeddings\n",
    "\n",
    "\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "\n",
    "    # Step 4: Rank documents by similarity score\n",
    "\n",
    "\n",
    "    ranked_indices = sorted(range(len(similarities)),\n",
    "                            key=lambda i: similarities[i], reverse=True)\n",
    "\n",
    "\n",
    "    # Step 5: Select top-k documents\n",
    "\n",
    "\n",
    "    ranked_docs = [retrieved_docs[i] for i in ranked_indices[:top_k]]\n",
    "\n",
    "\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_rag_chain(query, retriever, llm, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform retrieval-augmented generation with query optimization.\n",
    "    Args:\n",
    "        query (str): The user's input query.\n",
    "        retriever: The retrieval mechanism.\n",
    "        llm: The language model.\n",
    "        embeddings: The embedding model for ranking documents.\n",
    "        top_k (int): The number of top documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the generated response and retrieved documents.\n",
    "    \"\"\"\n",
    "    # Step 1: Expand the query using the LLM\n",
    "    expanded_queries = expand_query(query)\n",
    "\n",
    "    # Step 2: Retrieve documents using expanded queries\n",
    "    retrieved_docs = multi_query_retrieval(expanded_queries, retriever, top_k)\n",
    "\n",
    "    # Step 3: Rank documents using CoVe\n",
    "    ranked_docs = rank_with_cove(query, retrieved_docs, embeddings, top_k)\n",
    "\n",
    "    # Step 4: Combine the retrieved documents into a single context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in ranked_docs])\n",
    "\n",
    "    # Step 5: Define the system prompt with the context\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know.\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Question: {query}\"\n",
    "    )\n",
    "\n",
    "    # Step 6: Format the final prompt for the LLM\n",
    "    final_prompt = system_prompt.format(query=query)\n",
    "    # print(final_prompt)\n",
    "    # Step 7: Get the answer from the LLM\n",
    "    response = llm(final_prompt)  # Assuming llm takes input as a dictionary\n",
    "\n",
    "    return {\"query\": query, \"response\": response, \"retrieved_docs\": ranked_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_txt_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Small2Big Chunking Statistics ===\n",
      "Total Original Documents: 1\n",
      "Total Original Length: 6005 characters\n",
      "Total Chunks Created: 7\n",
      "Total Chunked Length: 6517 characters\n",
      "Average Chunk Size: 931.00 characters\n",
      "Minimum Chunk Size: 200 characters\n",
      "Maximum Chunk Size: 1000 characters\n"
     ]
    }
   ],
   "source": [
    "# Apply Small2Big chunking\n",
    "chunked_documents = small2big_chunking(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrievers\n",
    "bm25_retriever = BM25Retriever.from_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=chunked_documents, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combine retrievers using EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Queries: ['What are the key aspects to consider in privacy policies?', 'What are the important elements of privacy policies that I should pay attention to?', 'What are the main factors to focus on in privacy policies?']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanch\\AppData\\Local\\Temp\\ipykernel_27840\\3567895178.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)[:top_k]\n",
      "C:\\Users\\vanch\\AppData\\Local\\Temp\\ipykernel_27840\\4076487573.py:40: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(final_prompt)  # Assuming llm takes input as a dictionary\n"
     ]
    }
   ],
   "source": [
    "# List of queries\n",
    "queries = [\n",
    "    \"Tell me the main points I should care about privacy policies.\",\n",
    "]\n",
    "\n",
    "# Initialize LLM and embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize a list to store results\n",
    "results_list = []\n",
    "\n",
    "# Loop through each query, perform RAG, and store the results\n",
    "for query in queries:\n",
    "    result = optimized_rag_chain(\n",
    "        query=query,\n",
    "        retriever=ensemble_retriever,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        top_k=5,\n",
    "    )\n",
    "    # Append the result to the list\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query ===\n",
      "Tell me the main points I should care about privacy policies.\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "When considering privacy policies, here are the main points you should focus on:\n",
      "\n",
      "1. **Information Collection and Use**: Understand what types of personal information are collected (such as email, name, phone number, address, cookies, and usage data) and how this information is used.\n",
      "\n",
      "2. **Cookies**: Be aware of how cookies are used to enhance website experience and how you can control their use through browser settings.\n",
      "\n",
      "3. **Data Accuracy and Responsibilities**: Recognize the importance of providing accurate information and your responsibility to inform the entity of any changes to your personal data.\n",
      "\n",
      "4. **Data Security**: Ensure that the entity uses encryption and other security measures to protect your data both during transmission and while stored.\n",
      "\n",
      "5. **Data Retention and Disposal**: Know how long your data is retained and the process for data disposal once it is no longer needed.\n",
      "\n",
      "6. **Disclosure and Sharing**: Understand under what circumstances your data might be shared with third parties, such as service providers or in response to legal requests, and whether your data is shared for the benefit of third-party AI models.\n",
      "\n",
      "7. **Third-Party Websites**: Be aware that privacy practices of third-party websites linked from the entity's site are not the responsibility of the entity.\n",
      "\n",
      "8. **Changes to Privacy Policy**: Keep track of any updates to the privacy policy and how you will be informed of these changes.\n",
      "\n",
      "9. **Contact Information**: Know how to contact the entity if you have questions about their privacy policy.\n",
      "\n",
      "These points help you understand how your personal information is handled and what rights and responsibilities you have regarding your data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for result in results_list:\n",
    "    print(\"=== Query ===\")\n",
    "    print(result[\"query\"])\n",
    "    print(result[\"response\"].pretty_repr())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: COOKIES\n",
      "\n",
      "We use cookies to enhance your experience on our website. You can control the use of cookies through your web browser settings.\n",
      "\n",
      "THIRD-PARTY WEBSITES THIRD-PARTY WEBSITES\n",
      "\n",
      "Our website may contain links to third-party websites. We are not responsible for the privacy practices or content of those websites.\n",
      "\n",
      "CHANGES TO PRIVACY POLICY CHANGES TO PRIVACY POLICY\n",
      "\n",
      "We may update this Privacy Policy from time to time. The updated Privacy Policy will be posted on our website.\n",
      "\n",
      "CONTACT US CONTACT US\n",
      "\n",
      "If you have any questions about this Privacy Policy, please contact us through the customer portal or by email at\n",
      "\n",
      "PURPOSEFUL USE ONLY We commit to only use personal information for the purposes identified in the entity's privacy policy.\n",
      "Presight.io 2022 All Rights Reserved\n",
      "Ho Chi Minh City, Vietnam\n",
      "Singapore\n",
      "Seattle, WA, USA\n",
      "\n",
      "--- Document 2 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: By Role\n",
      "By Team\n",
      "\n",
      "PRIVACY POLICY\n",
      "\n",
      "\n",
      "LAST UPDATED 15 SEP 2023 At Presight, we are committed to protecting the privacy of our customers and visitors to our website. This Privacy Policy explains how we collect, use, and disclose information about our customers use, and disclose information about our customers and visitors. INFORMATION COLLECTION AND USE\n",
      "\n",
      "We collect several different types of information for various purposes to provide and improve our Service to you.\n",
      "\n",
      "TYPES OF DATA COLLECTED While using our Service, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you (\"Personal Data\"). Personally identifiable you (\"Personal Data\"). Personally identifiable information may include, but is not limited to: - Email address\n",
      "- First name and last name\n",
      "- Phone number\n",
      "- Address, State, Province, ZIP/Postal code, City\n",
      "- Cookies and Usage Data\n",
      "\n",
      "--- Document 3 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: QUALITY, INCLUDING DATA SUBJECTS' RESPONSIBILITIES FOR QUALITY - We are committed to maintaining the quality and accuracy of the personal information we collect and process.\n",
      "- We rely on data subjects to provide accurate and up-to-date information. - Data subjects have the responsibility to inform us of any changes or inaccuracies in their personal data. - If you believe that any information we hold about you is inaccurate, incomplete, or outdated, please contact us promptly to rectify the information. MONITORING AND ENFORCEMENT - We regularly monitor its data processing activities to ensure compliance with this privacy policy and applicable data protection laws. - In the event of a data breach or any unauthorized access to your personal information, we will notify you and the appropriate authorities as required by law. - We committed to cooperating with data protection authorities and complying with their advice and decisions regarding data protection and privacy matters.\n",
      "\n",
      "--- Document 4 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: help maintain data integrity and accuracy. You are encouraged to provide complete and valid information to ensure the smooth processing of their personal data. DISCLOSURE OF INFORMATION We may disclose your application data to third-party service providers who help us provide our services such as Datadog, AWS, Google Cloud and Google Workspace. We may also disclose your information Workspace. We may also disclose your information in response to a legal request, such as a subpoena or court order, or to protect our rights or the rights of others. SHARING OF PERSONAL DATA\n",
      "\n",
      "Your personal data will not be subject to sharing, transfer, rental or exchange for the benefit of third parties, including AI models. GOOGLE USER DATA AND GOOGLE WORKSPACE APIS In all cases when users authenticate the platform to Google Workspace, the following applies: - We do not retain or use Google User Data to develop, improve, or train generalized/non-personalized AI and/or ML models.\n",
      "\n",
      "--- Document 5 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: - We do not use Google Workspace APIs to develop, improve, or train generalized/non-personalized AI and/or ML models. - We do not transfer Google User Data to third-party AI tools for the purpose of developing, improving, or training generalized or non-personalized AI and/or ML models. DATA SECURITY - All data is encrypted both in transit and at rest, using industry-standard encryption methods. - We regularly perform security audits and vulnerability assessments to ensure the safety of our platform and the data stored within it. - Our employees are trained on best practices for data security, and access to customer data is restricted on a need-to-know basis. DATA RETENTION & DISPOSAL Customer data is retained for as long as the account is in active status. Data enters an “expired” state when the account is voluntarily closed. Expired account data will be retained for 60 days. account data will be retained for 60 days. After this period, the account and related data will be removed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results_list:\n",
    "    # Loop through the retrieved documents for this query\n",
    "    for i, doc in enumerate(result[\"retrieved_docs\"], start=1):\n",
    "        if isinstance(doc, Document):\n",
    "            print(f\"--- Document {i} ---\")\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "            print(f\"Content: {doc.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
